"use strict";(globalThis.webpackChunktmp_docusaurus_project=globalThis.webpackChunktmp_docusaurus_project||[]).push([[9815],{5269:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-5/project-2-visual-sorter","title":"Project 2: Visual Sorter Arm \u2013 Object Identification and Sorting","description":"\ud83c\udfaf Objective","source":"@site/docs/module-5/project-2-visual-sorter.md","sourceDirName":"module-5","slug":"/module-5/project-2-visual-sorter","permalink":"/docs/module-5/project-2-visual-sorter","draft":false,"unlisted":false,"editUrl":"https://github.com/EngineerAbdullahIqbal/roboailearner/tree/main/robotics_book_content/docs/module-5/project-2-visual-sorter.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Project 1: Sentient Sentry \u2013 Camera-Based Face Tracking","permalink":"/docs/module-5/project-1-sentient-sentry"},"next":{"title":"Project 3: Office Runner \u2013 Autonomous Item Delivery","permalink":"/docs/module-5/project-3-office-runner"}}');var t=o(4848),s=o(8453);const i={},a="Project 2: Visual Sorter Arm \u2013 Object Identification and Sorting",c={},l=[{value:"\ud83c\udfaf Objective",id:"-objective",level:3},{value:"\ud83e\udde0 Theory: Embodied Perception-Action Loop",id:"-theory-embodied-perception-action-loop",level:3},{value:"\ud83d\udee0\ufe0f Architecture",id:"\ufe0f-architecture",level:3},{value:"\ud83d\udcbb Implementation",id:"-implementation",level:3},{value:"1. Defining the Custom Service: <code>SorterControl.srv</code>",id:"1-defining-the-custom-service-sortercontrolsrv",level:4},{value:"2. <code>object_detector.py</code>: Perceiving the World",id:"2-object_detectorpy-perceiving-the-world",level:4},{value:"3. <code>arm_commander.py</code>: Acting on Perception",id:"3-arm_commanderpy-acting-on-perception",level:4},{value:"4. <code>sorter_bringup.launch.py</code>: Orchestrating the System",id:"4-sorter_bringuplaunchpy-orchestrating-the-system",level:4},{value:"\u26a0\ufe0f Common Pitfalls (Sim vs. Real)",id:"\ufe0f-common-pitfalls-sim-vs-real",level:3},{value:"\ud83e\uddea Verification",id:"-verification",level:3}];function d(e){const n={code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"project-2-visual-sorter-arm--object-identification-and-sorting",children:"Project 2: Visual Sorter Arm \u2013 Object Identification and Sorting"})}),"\n",(0,t.jsx)(n.h3,{id:"-objective",children:"\ud83c\udfaf Objective"}),"\n",(0,t.jsx)(n.p,{children:"Students will implement a ROS 2-based robotic arm system that identifies colored objects using OpenCV, transforms their coordinates using TF2, and sorts them into designated bins using MoveIt 2 for arm control in a simulated environment."}),"\n",(0,t.jsx)(n.h3,{id:"-theory-embodied-perception-action-loop",children:"\ud83e\udde0 Theory: Embodied Perception-Action Loop"}),"\n",(0,t.jsx)(n.p,{children:'In this project, we bridge the abstract world of computer vision with the physical reality of robot manipulation. The "Visual Sorter Arm" exemplifies an embodied perception-action loop:'}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": A camera (simulated) captures raw pixel data. The ",(0,t.jsx)(n.code,{children:"object_detector"}),' node processes this data to identify colored objects. This is analogous to a robot\'s "eyes."']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognition/Planning"}),": The ",(0,t.jsx)(n.code,{children:"object_detector"})," then broadcasts the detected object's position in a standardized ",(0,t.jsx)(n.code,{children:"object_frame"})," using TF2. The ",(0,t.jsx)(n.code,{children:"arm_commander"})," node subscribes to this information, transforms it into the robot's base frame, and plans a grasp trajectory using MoveIt 2. This is the robot's \"brain\" deciding how to act."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": MoveIt 2 generates a series of joint commands that are then sent to the robot's (simulated) joint controllers, causing the arm to physically move and sort the object. This is the robot's \"muscles.\""]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The success of this loop critically depends on low-latency perception, accurate coordinate transformations, and robust motion planning. Any delay or error in one stage can lead to a cascading failure, from missed detections to unintended collisions. The Quality of Service (QoS) for each ROS topic becomes paramount: sensor data might tolerate ",(0,t.jsx)(n.code,{children:"Best Effort"})," for speed, while control commands absolutely require ",(0,t.jsx)(n.code,{children:"Reliable"})," delivery for safety and precision."]}),"\n",(0,t.jsx)(n.h3,{id:"\ufe0f-architecture",children:"\ud83d\udee0\ufe0f Architecture"}),"\n",(0,t.jsxs)(n.p,{children:["The system comprises three main ROS 2 nodes: a simulated camera publisher (often part of the simulation environment), an ",(0,t.jsx)(n.code,{children:"object_detector"})," node, and an ",(0,t.jsx)(n.code,{children:"arm_commander"})," node, interacting via topics and services."]}),"\n",(0,t.jsx)(n.mermaid,{value:"graph LR\n    A[SimulatedCameraNode] --\x3e|/camera/color/image_raw| B(object_detector)\n    B -- TF2: Broadcast object_frame --\x3e TF[TF2 Transform Listener]\n    B -- /sorter_control/detect_object (request) --\x3e C(arm_commander)\n    C -- TF2: Listen for object_frame --\x3e TF\n    C -- MoveIt! Planning Service --\x3e D(MoveGroupNode)\n    D --\x3e|/joint_trajectory_controller/joint_trajectory| E[RobotJointControllers]\n    E --\x3e F[SimulatedRoboticArm]"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"ROS 2 Graph Explanation:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"SimulatedCameraNode"})}),": This node (often provided by Gazebo/Isaac Sim) publishes raw camera images on ",(0,t.jsx)(n.code,{children:"/camera/color/image_raw"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"object_detector"})}),": Subscribes to ",(0,t.jsx)(n.code,{children:"/camera/color/image_raw"}),", performs image processing (color segmentation), identifies object centroids, and then broadcasts a ",(0,t.jsx)(n.code,{children:"tf2"})," transform for each detected object (",(0,t.jsx)(n.code,{children:"object_frame"}),"). It also exposes a ROS 2 service ",(0,t.jsx)(n.code,{children:"/sorter_control/detect_object"})," that ",(0,t.jsx)(n.code,{children:"arm_commander"})," will call."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"arm_commander"})}),": Acts as the orchestrator. It calls the ",(0,t.jsx)(n.code,{children:"object_detector"})," service to trigger detection, listens for the ",(0,t.jsx)(n.code,{children:"object_frame"})," via ",(0,t.jsx)(n.code,{children:"tf2"}),", uses MoveIt 2 Python bindings to plan and execute pick-and-place trajectories, and sends commands to the ",(0,t.jsx)(n.code,{children:"MoveGroupNode"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"MoveGroupNode"})}),": This is a core MoveIt 2 component that provides motion planning services."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"RobotJointControllers"})}),": Receives joint trajectory commands from MoveIt 2 and controls the simulated robot arm's joints."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"SimulatedRoboticArm"})}),": The physical representation of the arm within the simulation environment."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-implementation",children:"\ud83d\udcbb Implementation"}),"\n",(0,t.jsxs)(n.p,{children:["The project will be organized into a ROS 2 package, typically named ",(0,t.jsx)(n.code,{children:"visual_sorter_arm"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"File Structure (Illustrative):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/visual_sorter_arm\n\u251c\u2500\u2500 visual_sorter_arm\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 object_detector.py\n\u2502   \u2514\u2500\u2500 arm_commander.py\n\u251c\u2500\u2500 srv\n\u2502   \u2514\u2500\u2500 SorterControl.srv\n\u251c\u2500\u2500 launch\n\u2502   \u2514\u2500\u2500 sorter_bringup.launch.py\n\u251c\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 sorter_moveit_config.yaml\n\u2514\u2500\u2500 package.xml\n\u2514\u2500\u2500 setup.py\n"})}),"\n",(0,t.jsxs)(n.h4,{id:"1-defining-the-custom-service-sortercontrolsrv",children:["1. Defining the Custom Service: ",(0,t.jsx)(n.code,{children:"SorterControl.srv"})]}),"\n",(0,t.jsxs)(n.p,{children:["This service will be used by ",(0,t.jsx)(n.code,{children:"arm_commander"})," to request object detection from ",(0,t.jsx)(n.code,{children:"object_detector"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Context"}),": ",(0,t.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/visual_sorter_arm/srv/SorterControl.srv"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"---\nbool success # true if object detected and transform broadcasted, false otherwise\ngeometry_msgs/msg/Point object_position # Position of the detected object in camera frame\nstring object_color # Color of the detected object\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"---"})," separates the request and response fields. In this case, there's no explicit request field; the call itself triggers detection."]}),"\n",(0,t.jsxs)(n.li,{children:["The response includes a ",(0,t.jsx)(n.code,{children:"success"})," boolean, the ",(0,t.jsx)(n.code,{children:"object_position"})," (in the camera's frame), and the ",(0,t.jsx)(n.code,{children:"object_color"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"2-object_detectorpy-perceiving-the-world",children:["2. ",(0,t.jsx)(n.code,{children:"object_detector.py"}),": Perceiving the World"]}),"\n",(0,t.jsx)(n.p,{children:"This node will perform color-based segmentation and broadcast the object's pose."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Context"}),": ",(0,t.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/visual_sorter_arm/visual_sorter_arm/object_detector.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport tf2_ros\nimport geometry_msgs.msg\nfrom visual_sorter_arm_interfaces.srv import SorterControl # Assuming interface package\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__(\'object_detector\')\n        self.bridge = CvBridge()\n        self.image_subscription = self.create_subscription(\n            Image,\n            \'/camera/color/image_raw\',\n            self.image_callback,\n            rclpy.qos.qos_profile_sensor_data # Best Effort for sensor data\n        )\n        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\n        self.service = self.create_service(\n            SorterControl,\n            \'sorter_control/detect_object\',\n            self.detect_object_callback\n        )\n        self.latest_image = None\n        self.get_logger().info(\'Object Detector Node has been started.\')\n\n    def image_callback(self, msg):\n        self.latest_image = msg\n\n    def detect_object_callback(self, request, response):\n        if self.latest_image is None:\n            self.get_logger().warn("No image received yet.")\n            response.success = False\n            return response\n\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(self.latest_image, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f"Error converting image: {e}")\n            response.success = False\n            return response\n\n        # --- Color Segmentation (Example for a red object) ---\n        hsv_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n        # Define range for red color (adjust as needed for your objects)\n        lower_red1 = np.array([0, 100, 100])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 100, 100])\n        upper_red2 = np.array([180, 255, 255])\n\n        mask1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n        mask2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n        red_mask = mask1 + mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Assume the largest contour is our object\n            largest_contour = max(contours, key=cv2.contourArea)\n            M = cv2.moments(largest_contour)\n            if M["m00"] > 0:\n                cx = int(M["m10"] / M["m00"])\n                cy = int(M["m01"] / M["m00"])\n\n                # --- Approximate 3D position (very basic example, depends on camera intrinsics) ---\n                # In a real scenario, you\'d use depth camera data or camera intrinsics to get Z.\n                # For simulation, we might assume a fixed Z or use known object height.\n                # This is a placeholder for actual depth calculation.\n                # For this example, let\'s just make up a \'z\' value relative to the camera.\n                # In simulation, you might have access to depth images or direct object poses.\n                # For simplicity, we\'ll assume a known object plane for rough estimation.\n                # A more robust solution involves camera intrinsics and depth images.\n                approx_z_distance = 0.5 # meters, example fixed distance from camera\n\n                # These need to be properly calculated based on camera FOV and intrinsics\n                # For this guide, we\'ll use placeholder values.\n                # Actual calculations would involve:\n                # fx, fy, cx, cy = camer-info.k[0], camer-info.k[4], camer-info.k[2], camer-info.k[5]\n                # x_camera = (cx_pixel - cx) * depth / fx\n                # y_camera = (cy_pixel - cy) * depth / fy\n\n                # Placeholder for X, Y in camera frame, assuming simple mapping\n                # These are NOT real-world accurate, just for demonstration\n                x_camera = (cx - cv_image.shape[1] / 2) * 0.001 # rough pixel to meter scale\n                y_camera = (cy - cv_image.shape[0] / 2) * 0.001 # rough pixel to meter scale\n\n                object_point_camera = geometry_msgs.msg.Point()\n                object_point_camera.x = approx_z_distance # x-forward in camera frame for RealSense\n                object_point_camera.y = -x_camera # y-left\n                object_point_camera.z = -y_camera # z-up\n\n\n                # --- Broadcast TF2 Transform ---\n                t = geometry_msgs.msg.TransformStamped()\n                t.header.stamp = self.get_clock().now().to_msg()\n                t.header.frame_id = self.latest_image.header.frame_id # Parent frame (e.g., \'camera_link\')\n                t.child_frame_id = "detected_object_frame"\n                t.transform.translation.x = object_point_camera.x\n                t.transform.translation.y = object_point_camera.y\n                t.transform.translation.z = object_point_camera.z\n                t.transform.rotation.x = 0.0 # No rotation for now, assume upright\n                t.transform.rotation.y = 0.0\n                t.transform.rotation.z = 0.0\n                t.transform.rotation.w = 1.0 # Identity quaternion\n                self.tf_broadcaster.sendTransform(t)\n\n                self.get_logger().info(f"Detected object at ({object_point_camera.x:.2f}, {object_point_camera.y:.2f}, {object_point_camera.z:.2f}) in camera frame.")\n\n                response.success = True\n                response.object_position = object_point_camera\n                response.object_color = "red" # For now, hardcode detected color\n            else:\n                self.get_logger().info("No significant red object found.")\n                response.success = False\n        else:\n            self.get_logger().info("No red objects detected.")\n            response.success = False\n\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    object_detector = ObjectDetector()\n    rclpy.spin(object_detector)\n    object_detector.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n\n# setup.py entry point (add to your setup.py in \'entry_points\' section):\n# \'visual_sorter_arm = visual_sorter_arm.object_detector:main\',\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Physical Context ",(0,t.jsx)(n.code,{children:"object_detector.py"})]}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Target"}),": This code, especially the OpenCV part, runs on the main CPU. If integrated with a camera driver, it would typically run on the robot's onboard computer (e.g., Jetson Orin Nano)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Constraint"}),": The frame rate of the camera image topic and the processing time for ",(0,t.jsx)(n.code,{children:"cv2.inRange"})," and ",(0,t.jsx)(n.code,{children:"cv2.findContours"})," are critical. If ",(0,t.jsx)(n.code,{children:"object_detector"})," cannot keep up with the camera's frame rate, it will fall behind, leading to outdated object poses."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"QoS"}),": ",(0,t.jsx)(n.code,{children:"rclpy.qos.qos_profile_sensor_data"})," is used for the image subscription, which is typically ",(0,t.jsx)(n.code,{children:"Best Effort"})," to prioritize fresh data over guaranteed delivery, reducing latency for perception. This is crucial for a responsive perception-action loop."]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"3-arm_commanderpy-acting-on-perception",children:["3. ",(0,t.jsx)(n.code,{children:"arm_commander.py"}),": Acting on Perception"]}),"\n",(0,t.jsx)(n.p,{children:"This node will utilize MoveIt 2 to plan and execute arm movements."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Context"}),": ",(0,t.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/visual_sorter_arm/visual_sorter_arm/arm_commander.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport moveit_commander\nimport moveit_msgs.msg\nimport geometry_msgs.msg\nimport tf2_ros\nfrom tf2_ros import LookupException, ConnectivityException, ExtrapolationException\nfrom visual_sorter_arm_interfaces.srv import SorterControl # Assuming interface package\nimport sys\nimport time\n\nclass ArmCommander(Node):\n    def __init__(self):\n        super().__init__(\'arm_commander\')\n\n        # Initialize MoveIt! Commander\n        # Note: moveit_commander.roscpp_initialize(sys.argv) is for ROS 1.\n        # For ROS 2, it\'s integrated with rclpy.init(), and MoveIt! Commander\n        # works by setting up the node in the background.\n        moveit_commander.roscpp_initialize(sys.argv) # This needs to be called to initialize internal ROS 2 components for MoveIt\n        self.robot = moveit_commander.RobotCommander(node_name=\'arm_commander_moveit\')\n        self.scene = moveit_commander.PlanningSceneInterface(synchronous=True, node_name=\'arm_commander_moveit\')\n        self.group_name = "arm" # Replace with your MoveIt! planning group name\n        self.move_group = moveit_commander.MoveGroupCommander(self.group_name, node_name=\'arm_commander_moveit\')\n\n        # Display trajectory publisher\n        self.display_trajectory_publisher = self.create_publisher(\n            moveit_msgs.msg.DisplayTrajectory,\n            \'/move_group/display_planned_path\',\n            10 # QoS depth\n        )\n\n        # TF2 Buffer and Listener\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # Object Detector Service Client\n        self.sorter_service_client = self.create_client(\n            SorterControl,\n            \'sorter_control/detect_object\'\n        )\n        while not self.sorter_service_client.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info(\'Object Detector service not available, waiting...\')\n\n        self.get_logger().info(\'Arm Commander Node has been started.\')\n\n    def call_object_detector_service(self):\n        self.get_logger().info("Calling object detector service...")\n        request = SorterControl.Request()\n        future = self.sorter_service_client.call_async(request)\n        rclpy.spin_until_future_complete(self, future)\n        return future.result()\n\n    def go_to_pose(self, pose_goal):\n        self.move_group.set_pose_target(pose_goal)\n        plan = self.move_group.plan()\n\n        if plan and plan[1].joint_trajectory.points: # Check if plan is not empty and has points\n            self.get_logger().info("Planning successful. Executing motion...")\n            self.move_group.execute(plan[1], wait=True)\n            self.move_group.stop() # Ensure no residual movement\n            self.move_group.clear_pose_targets()\n            return True\n        else:\n            self.get_logger().warn("Motion planning failed.")\n            return False\n\n    def pick_and_place(self):\n        # 1. Go to pre-grasp pose (e.g., home position or above the table)\n        # This should be defined as a named pose in your SRDF or calculated.\n        self.move_group.set_named_target("home") # Assuming \'home\' is a defined pose\n        self.move_group.go(wait=True)\n        self.move_group.stop()\n        self.move_group.clear_pose_targets()\n\n        # 2. Call the object detector service\n        response = self.call_object_detector_service()\n\n        if response.success:\n            self.get_logger().info(f"Object detected: {response.object_color} at {response.object_position}")\n\n            # --- TF2: Transform object_frame to base_link ---\n            try:\n                # Wait for the transform to become available\n                # It\'s crucial that object_detector is broadcasting frequently\n                t_stamped = self.tf_buffer.lookup_transform(\n                    self.move_group.get_planning_frame(), # Target frame, typically \'base_link\'\n                    "detected_object_frame", # Source frame\n                    rclpy.time.Time() # Get the latest transform\n                )\n                self.get_logger().info(f"Transform received from {t_stamped.header.frame_id} to {t_stamped.child_frame_id}")\n\n                object_pose_base = geometry_msgs.msg.Pose()\n                object_pose_base.position.x = t_stamped.transform.translation.x\n                object_pose_base.position.y = t_stamped.transform.translation.y\n                object_pose_base.position.z = t_stamped.transform.translation.z\n                object_pose_base.orientation = t_stamped.transform.rotation\n\n                self.get_logger().info(f"Object in base_link frame: {object_pose_base.position}")\n\n                # 3. Plan grasp pose (above object, then down to object)\n                # Define offset for pre-grasp (above the object)\n                pre_grasp_offset = 0.15 # 15 cm above object\n                grasp_offset = 0.05 # 5 cm from object for gripper\n\n                # Pre-grasp pose\n                pre_grasp_pose = geometry_msgs.msg.Pose()\n                pre_grasp_pose.position.x = object_pose_base.position.x\n                pre_grasp_pose.position.y = object_pose_base.position.y\n                pre_grasp_pose.position.z = object_pose_base.position.z + pre_grasp_offset\n                pre_grasp_pose.orientation.w = 1.0 # Assuming simple approach from top\n\n                # Grasp pose\n                grasp_pose = geometry_msgs.msg.Pose()\n                grasp_pose.position.x = object_pose_base.position.x\n                grasp_pose.position.y = object_pose_base.position.y\n                grasp_pose.position.z = object_pose_base.position.z + grasp_offset\n                grasp_pose.orientation.w = 1.0 # Assuming simple approach from top\n\n                # Move to pre-grasp\n                if self.go_to_pose(pre_grasp_pose):\n                    self.get_logger().info("Reached pre-grasp pose.")\n                    # Move to grasp\n                    if self.go_to_pose(grasp_pose):\n                        self.get_logger().info("Reached grasp pose. Gripping object...")\n                        # TODO: Add gripper open/close logic here (publish to gripper topic)\n                        time.sleep(1.0) # Simulate gripper closing\n\n                        # 4. Lift object\n                        lift_pose = geometry_msgs.msg.Pose()\n                        lift_pose.position.x = grasp_pose.position.x\n                        lift_pose.position.y = grasp_pose.position.y\n                        lift_pose.position.z = grasp_pose.position.z + pre_grasp_offset\n                        lift_pose.orientation.w = 1.0\n                        if self.go_to_pose(lift_pose):\n                            self.get_logger().info("Lifted object.")\n\n                            # 5. Move to sorting bin location (example for \'red\' bin)\n                            # Define bin locations as named poses or fixed coordinates\n                            if response.object_color == "red":\n                                self.move_group.set_named_target("red_bin_pose") # Assuming \'red_bin_pose\' is defined\n                                if self.move_group.go(wait=True):\n                                    self.get_logger().info("Reached red bin. Releasing object...")\n                                    # TODO: Add gripper open/close logic here\n                                    time.sleep(1.0) # Simulate gripper opening\n                                    return_pose = geometry_msgs.msg.Pose()\n                                    return_pose.position.x = self.move_group.get_current_pose().pose.position.x\n                                    return_pose.position.y = self.move_group.get_current_pose().pose.position.y\n                                    return_pose.position.z = self.move_group.get_current_pose().pose.position.z + pre_grasp_offset # Move up after release\n                                    return_pose.orientation.w = 1.0\n                                    self.go_to_pose(return_pose) # Lift slightly after release\n                                    self.get_logger().info("Object released.")\n                            else:\n                                self.get_logger().warn(f"No bin defined for color: {response.object_color}")\n\n                # 6. Return to home or ready position\n                self.move_group.set_named_target("home")\n                self.move_group.go(wait=True)\n                self.move_group.stop()\n                self.move_group.clear_pose_targets()\n\n            except (LookupException, ConnectivityException, ExtrapolationException) as ex:\n                self.get_logger().error(f"TF2 transform error: {ex}")\n            except Exception as e:\n                self.get_logger().error(f"An error occurred during pick and place: {e}")\n        else:\n            self.get_logger().info("No object detected or service failed.")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    arm_commander = ArmCommander()\n    # For demonstration, we can trigger the pick and place loop here\n    try:\n        while rclpy.ok():\n            arm_commander.pick_and_place()\n            time.sleep(5) # Wait before next sorting attempt\n    except KeyboardInterrupt:\n        pass\n    finally:\n        arm_commander.destroy_node()\n        moveit_commander.roscpp_shutdown() # Clean up MoveIt! Commander\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n\n# setup.py entry point (add to your setup.py in \'entry_points\' section):\n# \'arm_commander = visual_sorter_arm.arm_commander:main\',\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Physical Context ",(0,t.jsx)(n.code,{children:"arm_commander.py"})]}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Target"}),": This node, due to MoveIt 2's computational demands (motion planning, collision checking), generally requires a more powerful CPU, likely running on a workstation for simulation. In a real-world scenario on a Jetson Orin, performance optimization and reduced planning complexity would be crucial."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Constraint"}),": Motion planning latency is critical. If planning takes too long, the perception data might become stale, leading to movements based on outdated object positions. TF2 lookup latency is also a factor; waiting for transforms adds delay."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Incorrectly planned movements or unexpected collisions are a major risk, even in simulation. The ",(0,t.jsx)(n.code,{children:"go_to_pose"})," function includes ",(0,t.jsx)(n.code,{children:"move_group.stop()"})," and ",(0,t.jsx)(n.code,{children:"clear_pose_targets()"})," to ensure the arm comes to a complete halt and clears previous targets."]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"4-sorter_bringuplaunchpy-orchestrating-the-system",children:["4. ",(0,t.jsx)(n.code,{children:"sorter_bringup.launch.py"}),": Orchestrating the System"]}),"\n",(0,t.jsxs)(n.p,{children:["This launch file will start the ",(0,t.jsx)(n.code,{children:"object_detector"})," and ",(0,t.jsx)(n.code,{children:"arm_commander"})," nodes, along with the necessary simulation and MoveIt 2 components."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Context"}),": ",(0,t.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/visual_sorter_arm/launch/sorter_bringup.launch.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PythonExpression\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.conditions import IfCondition\nimport yaml\n\ndef generate_launch_description():\n    # Get package directories\n    visual_sorter_arm_dir = get_package_share_directory('visual_sorter_arm')\n    moveit_config_dir = get_package_share_directory('YOUR_ROBOT_moveit_config') # Replace with your robot's MoveIt config package\n\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    # Use a specific robot model in Gazebo, e.g., from an environment variable or default\n    robot_xacro_file = LaunchConfiguration('robot_xacro_file', default=os.path.join(moveit_config_dir, 'config', 'YOUR_ROBOT.urdf.xacro')) # Replace\n\n    # Load MoveIt! planning pipeline configuration\n    # This assumes your robot's MoveIt! config package has a 'config' directory with planning.yaml\n    # and other MoveIt! related files.\n    # The actual path depends on how your MoveIt! config package is structured.\n    try:\n        # Example: Loading common MoveIt! configs if available in the robot's MoveIt config package\n        kinematics_yaml = os.path.join(moveit_config_dir, 'config', 'kinematics.yaml')\n        joint_limits_yaml = os.path.join(moveit_config_dir, 'config', 'joint_limits.yaml')\n        pilz_cartesian_limits_yaml = os.path.join(moveit_config_dir, 'config', 'pilz_cartesian_limits.yaml')\n        ompl_planning_yaml = os.path.join(moveit_config_dir, 'config', 'ompl_planning.yaml')\n\n        with open(ompl_planning_yaml, 'r') as file:\n            ompl_planning_config = yaml.safe_load(file)\n\n    except FileNotFoundError as e:\n        print(f\"Error loading MoveIt! config files: {e}. Please ensure YOUR_ROBOT_moveit_config package is correctly set up.\")\n        return LaunchDescription([]) # Return an empty launch description to prevent errors\n\n    # Node for the object detector\n    object_detector_node = Node(\n        package='visual_sorter_arm',\n        executable='object_detector',\n        name='object_detector',\n        output='screen',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Node for the arm commander\n    arm_commander_node = Node(\n        package='visual_sorter_arm',\n        executable='arm_commander',\n        name='arm_commander',\n        output='screen',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            ompl_planning_config # Pass MoveIt! config parameters\n            # Add other MoveIt! related parameters here if needed\n        ]\n    )\n\n    # Include the robot's MoveIt! launch file for core MoveIt! nodes and controllers\n    # This will typically start RViz, the move_group node, joint_state_publisher, etc.\n    # Replace 'YOUR_ROBOT_moveit_config' and 'launch_moveit.launch.py' with your actual package and launch file.\n    move_group_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([os.path.join(\n            moveit_config_dir, 'launch', 'YOUR_ROBOT_moveit.launch.py')]), # Typically starts move_group, rviz, etc.\n        launch_arguments={\n            'use_sim_time': use_sim_time,\n            'robot_xacro_file': robot_xacro_file,\n            # Pass other arguments required by your robot's MoveIt launch file\n        }.items()\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'robot_xacro_file',\n            default_value=os.path.join(moveit_config_dir, 'config', 'YOUR_ROBOT.urdf.xacro'),\n            description='Path to the robot URDF.xacro file'\n        ),\n        object_detector_node,\n        arm_commander_node,\n        move_group_launch\n    ])\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Physical Context ",(0,t.jsx)(n.code,{children:"sorter_bringup.launch.py"})]}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Magic" Commands Correction'}),": This launch file is not magic. It systematically starts several key components:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"object_detector"})," node for vision processing."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"arm_commander"})," node for control logic."]}),"\n",(0,t.jsxs)(n.li,{children:["An ",(0,t.jsx)(n.code,{children:"IncludeLaunchDescription"})," which pulls in the MoveIt 2 launch file for your specific robot. This typically launches the ",(0,t.jsx)(n.code,{children:"move_group"})," node (the core MoveIt 2 server), ",(0,t.jsx)(n.code,{children:"robot_state_publisher"}),", ",(0,t.jsx)(n.code,{children:"joint_state_publisher"}),", and optionally RViz for visualization."]}),"\n",(0,t.jsxs)(n.li,{children:["Parameters like ",(0,t.jsx)(n.code,{children:"use_sim_time"})," are crucial for synchronizing nodes with the simulation clock."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dependencies"}),": This launch file explicitly depends on your robot's MoveIt 2 configuration package (e.g., ",(0,t.jsx)(n.code,{children:"YOUR_ROBOT_moveit_config"}),") for URDF, SRDF, and planning parameters. Without a correctly configured MoveIt 2 setup for your robot, this launch file will fail."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"\ufe0f-common-pitfalls-sim-vs-real",children:"\u26a0\ufe0f Common Pitfalls (Sim vs. Real)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"What works in Isaac Sim"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Perfect sensor data (no noise, consistent lighting)."}),"\n",(0,t.jsx)(n.li,{children:"Instantaneous command execution (minimal latency)."}),"\n",(0,t.jsx)(n.li,{children:"Perfect odometry and joint encoders."}),"\n",(0,t.jsx)(n.li,{children:"Easy object spawning and precise pose control for testing."}),"\n",(0,t.jsx)(n.li,{children:"MoveIt 2 planning often succeeds quickly due to simplified physics."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"What fails on the physical robot"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Noise/Lighting"}),": Real cameras are affected by ambient light, reflections, shadows, and motion blur during rapid movements. This can severely degrade ",(0,t.jsx)(n.code,{children:"object_detector"}),"'s performance, leading to missed detections or incorrect object centroids."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"}),": The end-to-end latency from camera capture, image processing, TF2 lookup, MoveIt 2 planning, to actual motor command execution is significantly higher in reality. A 100ms delay in planning might mean the object has moved."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TF2 Drift"}),": Real robot base links drift over time relative to the map/odom frame. If ",(0,t.jsx)(n.code,{children:"object_detector"})," broadcasts transforms based on an inaccurate ",(0,t.jsx)(n.code,{children:"camera_link"})," pose, the ",(0,t.jsx)(n.code,{children:"object_frame"})," will also be inaccurate."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MoveIt 2 Success Rate"}),": MoveIt 2 planning can be slower and less robust on edge devices. Complex environments or tight joint limits might lead to frequent planning failures (",(0,t.jsx)(n.code,{children:"Motion planning failed."})," messages)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasping"}),': Real-world grasping is much harder due to friction, object compliance, gripper force control, and precise alignment. A simple "go to pose" in simulation is an advanced operation in reality.']}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fix"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor/Perception"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement robust vision algorithms (e.g., adaptive thresholding, Kalman filters for object tracking, depth image processing for 3D localization)."}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate camera settings (exposure, gain) for the environment."}),"\n",(0,t.jsx)(n.li,{children:"Consider dedicated hardware for vision processing (e.g., NVIDIA Jetson's integrated GPU for accelerated OpenCV or AI inference)."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optimize code for performance (e.g., use C++ for critical vision components, leverage GPU acceleration)."}),"\n",(0,t.jsx)(n.li,{children:"Minimize network hops and use wired connections where possible."}),"\n",(0,t.jsx)(n.li,{children:"Implement predictive control or reactive behaviors for small corrections."}),"\n",(0,t.jsxs)(n.li,{children:["Tune QoS profiles: ",(0,t.jsx)(n.code,{children:"Reliable"})," for critical commands (e.g., ",(0,t.jsx)(n.code,{children:"joint_trajectory_controller/joint_trajectory"}),"), ",(0,t.jsx)(n.code,{children:"Best Effort"})," for high-frequency, less critical data (e.g., ",(0,t.jsx)(n.code,{children:"camera/color/image_raw"}),")."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TF2"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a robust SLAM (Simultaneous Localization and Mapping) or odometry system to provide accurate and consistent robot base pose."}),"\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.code,{children:"tf2_ros.TransformListener.wait_for_transform()"})," with reasonable timeouts, but be aware it adds latency."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MoveIt 2"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simplify planning scene (remove unnecessary collision objects)."}),"\n",(0,t.jsx)(n.li,{children:"Use simpler planning algorithms or reduce planning attempts/time."}),"\n",(0,t.jsx)(n.li,{children:"Implement error handling for planning failures (e.g., retry planning, move to a safe home position)."}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Crucial Safety"}),": Always test motion plans in simulation thoroughly before deploying to a real robot. Set joint limits and velocity/acceleration limits conservatively."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasping"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Utilize force/torque sensors on the gripper for compliant grasping."}),"\n",(0,t.jsx)(n.li,{children:"Implement visual servoing or tactile feedback for fine alignment."}),"\n",(0,t.jsx)(n.li,{children:"Consider vacuum grippers for simpler pick-and-place if applicable."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-verification",children:"\ud83e\uddea Verification"}),"\n",(0,t.jsxs)(n.p,{children:["To verify the ",(0,t.jsx)(n.code,{children:"object_detector"})," and ",(0,t.jsx)(n.code,{children:"arm_commander"})," nodes are functioning correctly:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Start your simulation environment and robot's MoveIt 2 stack"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 launch YOUR_ROBOT_moveit_config YOUR_ROBOT_moveit.launch.py # Replace with your robot's actual launch\nros2 launch gazebo_ros gazebo.launch.py # If using Gazebo\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Launch your visual sorter arm nodes"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 launch visual_sorter_arm sorter_bringup.launch.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Verify ",(0,t.jsx)(n.code,{children:"object_detector"})," is receiving images"]}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic hz /camera/color/image_raw\n"})}),"\n",(0,t.jsx)(n.p,{children:"You should see a non-zero frequency."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Verify ",(0,t.jsx)(n.code,{children:"object_detector"})," is broadcasting transforms"]}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 run tf2_ros tf2_echo camera_link detected_object_frame\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If an object is detected, you should see continuous transforms being published between ",(0,t.jsx)(n.code,{children:"camera_link"})," and ",(0,t.jsx)(n.code,{children:"detected_object_frame"}),". This command shows the (x,y,z) position and (x,y,z,w) quaternion orientation of ",(0,t.jsx)(n.code,{children:"detected_object_frame"})," relative to ",(0,t.jsx)(n.code,{children:"camera_link"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:["Observe ",(0,t.jsx)(n.code,{children:"arm_commander"})," logs for planning and execution"]}),":\nWatch the terminal where ",(0,t.jsx)(n.code,{children:"arm_commander"})," is running. You should see messages like:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Calling object detector service..."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Object detected: red at ..."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Transform received..."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Planning successful. Executing motion..."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Reached pre-grasp pose."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Lifted object."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"Reached red bin. Releasing object..."})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Visualize in RViz"}),":\nIn RViz (which should be launched by your robot's MoveIt 2 launch file), you should see:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The robot arm moving to the detected object's location."}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"detected_object_frame"})," in the TF display if an object is present."]}),"\n",(0,t.jsx)(n.li,{children:"Planned trajectories visualized before execution."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Outcomes"}),":\nUpon successful execution, a student should observe:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"In the simulation environment (Gazebo/Isaac Sim), a simulated robotic arm identifying colored objects (e.g., red cubes) that have been spawned."}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"object_detector"})," node successfully broadcasting a TF2 transform for the ",(0,t.jsx)(n.code,{children:"detected_object_frame"})," representing the object's position."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"arm_commander"})," node calling the ",(0,t.jsx)(n.code,{children:"object_detector"})," service, receiving the object's information, and using MoveIt 2 to plan and execute a path to pick up the detected object."]}),"\n",(0,t.jsx)(n.li,{children:'The robotic arm moving to the object, simulating a grasp, lifting it, moving it to a designated "red bin" location, and releasing it.'}),"\n",(0,t.jsx)(n.li,{children:"All these operations should be reflected in the console output of the respective nodes and visually confirmed in RViz."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>a});var r=o(6540);const t={},s=r.createContext(t);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);