"use strict";(globalThis.webpackChunktmp_docusaurus_project=globalThis.webpackChunktmp_docusaurus_project||[]).push([[2430],{91:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-5/project-1-sentient-sentry","title":"Project 1: Sentient Sentry \u2013 Camera-Based Face Tracking","description":"\ud83c\udfaf Objective","source":"@site/docs/module-5/project-1-sentient-sentry.md","sourceDirName":"module-5","slug":"/module-5/project-1-sentient-sentry","permalink":"/docs/module-5/project-1-sentient-sentry","draft":false,"unlisted":false,"editUrl":"https://github.com/EngineerAbdullahIqbal/roboailearner/tree/main/robotics_book_content/docs/module-5/project-1-sentient-sentry.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Student Projects: Hands-on Physical AI Robotics","permalink":"/docs/module-5/chapter-14-student-projects"},"next":{"title":"Project 2: Visual Sorter Arm \u2013 Object Identification and Sorting","permalink":"/docs/module-5/project-2-visual-sorter"}}');var i=r(4848),o=r(8453);const a={},s="Project 1: Sentient Sentry \u2013 Camera-Based Face Tracking",l={},c=[{value:"\ud83c\udfaf Objective",id:"-objective",level:3},{value:"\ud83e\udde0 Theory: Vision-Based Feedback Control",id:"-theory-vision-based-feedback-control",level:3},{value:"\ud83d\udee0\ufe0f Architecture",id:"\ufe0f-architecture",level:3},{value:"\ud83d\udcbb Implementation",id:"-implementation",level:3},{value:"1. Create ROS 2 Python Package",id:"1-create-ros-2-python-package",level:4},{value:"2. Configuration File (<code>params.yaml</code>)",id:"2-configuration-file-paramsyaml",level:4},{value:"3. Vision Node (<code>vision_node.py</code>)",id:"3-vision-node-vision_nodepy",level:4},{value:"4. Control Node (<code>control_node.py</code>)",id:"4-control-node-control_nodepy",level:4},{value:"5. Launch File (<code>sentient_sentry.launch.py</code>)",id:"5-launch-file-sentient_sentrylaunchpy",level:4},{value:"6. Update <code>setup.py</code>",id:"6-update-setuppy",level:4},{value:"\u26a0\ufe0f Common Pitfalls (Sim vs. Real)",id:"\ufe0f-common-pitfalls-sim-vs-real",level:3},{value:"\ud83e\uddea Verification",id:"-verification",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"project-1-sentient-sentry--camera-based-face-tracking",children:"Project 1: Sentient Sentry \u2013 Camera-Based Face Tracking"})}),"\n",(0,i.jsx)(n.h3,{id:"-objective",children:"\ud83c\udfaf Objective"}),"\n",(0,i.jsx)(n.p,{children:"Students will implement a ROS 2-based system that uses a simulated camera to detect faces and controls a simulated robot (or a pan-tilt unit) to keep the detected face centered in the camera's view. This project introduces core ROS 2 concepts, basic computer vision with OpenCV, and fundamental PID control."}),"\n",(0,i.jsx)(n.h3,{id:"-theory-vision-based-feedback-control",children:"\ud83e\udde0 Theory: Vision-Based Feedback Control"}),"\n",(0,i.jsx)(n.p,{children:"The Sentient Sentry project is a practical application of a vision-based feedback control loop. In essence, the robot acts as a closed-loop system:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception (VisionNode)"}),': The robot\'s "eyes" (a simulated camera) capture an image. This image is processed to detect faces and calculate their position relative to the image center.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cognition (Error Calculation)"}),": The ",(0,i.jsx)(n.code,{children:"VisionNode"}),' then quantifies the "error" \u2013 how far off-center the detected face is. This error is the input to the control system.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action (ControlNode)"}),': The robot\'s "brain" (',(0,i.jsx)(n.code,{children:"ControlNode"}),") uses this error to generate a control command (angular velocity). A PID (Proportional-Integral-Derivative) controller is employed to calculate this command, aiming to minimize the error and bring the face back to the center."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physical Response"}),": The simulated robot (or pan-tilt unit) executes the angular velocity command, causing it to turn."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:['This loop repeats continuously, allowing the robot to "track" the face. The ',(0,i.jsx)(n.strong,{children:"Quality of Service (QoS)"})," of ROS 2 topics is crucial here: ",(0,i.jsx)(n.code,{children:"Best Effort"})," for camera images prioritizes speed, while ",(0,i.jsx)(n.code,{children:"Reliable"})," for control commands ensures critical instructions are not lost."]}),"\n",(0,i.jsx)(n.h3,{id:"\ufe0f-architecture",children:"\ud83d\udee0\ufe0f Architecture"}),"\n",(0,i.jsxs)(n.p,{children:["The system consists of two primary ROS 2 Python nodes: ",(0,i.jsx)(n.code,{children:"vision_node"})," and ",(0,i.jsx)(n.code,{children:"control_node"}),", communicating via a custom ",(0,i.jsx)(n.code,{children:"/sentry/target_error"})," topic."]}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\n    A[SimulatedCameraNode] --\x3e|/image_raw (sensor_msgs/Image)| B(VisionNode)\n    B --\x3e|/sentry/target_error (std_msgs/Float32)| C(ControlNode)\n    C --\x3e|/cmd_vel (geometry_msgs/Twist)| D[RobotBaseController]\n    D --\x3e E[SimulatedRobot]\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#add8e6,stroke:#333,stroke-width:2px\n    style C fill:#ccffcc,stroke:#333,stroke-width:2px\n    style D fill:#ffcc99,stroke:#333,stroke-width:2px\n    style E fill:#ffff99,stroke:#333,stroke-width:2px"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ROS 2 Graph Explanation:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"SimulatedCameraNode"})}),": This node (typically part of your simulation environment, e.g., a Gazebo camera plugin) publishes raw image data on the ",(0,i.jsx)(n.code,{children:"/image_raw"})," topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"VisionNode"})}),": Subscribes to ",(0,i.jsx)(n.code,{children:"/image_raw"}),", processes images using OpenCV for face detection, calculates the horizontal offset (error) of the detected face from the image center, and publishes this error on ",(0,i.jsx)(n.code,{children:"/sentry/target_error"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"ControlNode"})}),": Subscribes to ",(0,i.jsx)(n.code,{children:"/sentry/target_error"}),", implements a PID control algorithm, and publishes the calculated angular velocity commands on ",(0,i.jsx)(n.code,{children:"/cmd_vel"})," (a standard topic for robot base control)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"RobotBaseController"})}),": This is the interface to your simulated robot's motors (e.g., a Gazebo differential drive controller). It receives ",(0,i.jsx)(n.code,{children:"/cmd_vel"})," commands and translates them into motor actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"SimulatedRobot"})}),": The visual and physical representation of your robot within the simulation environment."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"-implementation",children:"\ud83d\udcbb Implementation"}),"\n",(0,i.jsxs)(n.p,{children:["The project is structured as a single ROS 2 Python package named ",(0,i.jsx)(n.code,{children:"sentient_sentry"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"File Structure:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/sentient_sentry\n\u251c\u2500\u2500 sentient_sentry\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 vision_node.py\n\u2502   \u2514\u2500\u2500 control_node.py\n\u251c\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 params.yaml\n\u251c\u2500\u2500 launch\n\u2502   \u2514\u2500\u2500 sentient_sentry.launch.py\n\u251c\u2500\u2500 package.xml\n\u2514\u2500\u2500 setup.py\n"})}),"\n",(0,i.jsx)(n.h4,{id:"1-create-ros-2-python-package",children:"1. Create ROS 2 Python Package"}),"\n",(0,i.jsxs)(n.p,{children:["To create the ROS 2 Python package, open your terminal and navigate to your ROS 2 workspace ",(0,i.jsx)(n.code,{children:"src"})," directory. Then, run the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 pkg create sentient_sentry --build-type ament_python --dependencies rclpy cv_bridge sensor_msgs geometry_msgs std_msgs\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This command creates a new Python package named ",(0,i.jsx)(n.code,{children:"sentient_sentry"})," with the necessary build type and declares its dependencies on ",(0,i.jsx)(n.code,{children:"rclpy"})," (ROS Client Library for Python), ",(0,i.jsx)(n.code,{children:"cv_bridge"})," (for OpenCV-ROS image conversion), ",(0,i.jsx)(n.code,{children:"sensor_msgs"})," (for image messages), ",(0,i.jsx)(n.code,{children:"geometry_msgs"})," (for Twist velocity messages), and ",(0,i.jsx)(n.code,{children:"std_msgs"})," (for basic data types like Float32)."]}),"\n",(0,i.jsxs)(n.h4,{id:"2-configuration-file-paramsyaml",children:["2. Configuration File (",(0,i.jsx)(n.code,{children:"params.yaml"}),")"]}),"\n",(0,i.jsxs)(n.p,{children:["Create a ",(0,i.jsx)(n.code,{children:"config"})," directory inside your ",(0,i.jsx)(n.code,{children:"sentient_sentry"})," package and add ",(0,i.jsx)(n.code,{children:"params.yaml"}),". This file will store our PID gains and other node-specific parameters, allowing for easy tuning without recompiling code."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context"}),": ",(0,i.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/sentient_sentry/config/params.yaml"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# PID Controller Gains\n# These values are critical for robot stability and responsiveness.\n# Improper tuning can lead to oscillations or sluggish behavior.\npid:\n  kp: 0.5  # Proportional gain: Determines the response to the current error.\n  ki: 0.0  # Integral gain: Addresses accumulated error over time (helps eliminate steady-state error).\n  kd: 0.0  # Derivative gain: Damps oscillations by considering the rate of change of error.\n\n# Vision Node Parameters\nvision:\n  camera_topic: "/image_raw" # The topic from which the camera images are received.\n  # Path to the Haar Cascade XML file for face detection.\n  # You might need to adjust this path based on your OpenCV installation.\n  # For simulation, ensure this file is accessible within the ROS environment.\n  face_cascade_path: "/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml"\n  image_width: 640 # Expected width of the camera image. Used for centering calculations.\n  image_height: 480 # Expected height of the camera image.\n'})}),"\n",(0,i.jsxs)(n.h4,{id:"3-vision-node-vision_nodepy",children:["3. Vision Node (",(0,i.jsx)(n.code,{children:"vision_node.py"}),")"]}),"\n",(0,i.jsx)(n.p,{children:"This node subscribes to the camera's raw image topic, uses OpenCV to detect faces, and publishes the horizontal error of the detected face from the image center."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context"}),": ",(0,i.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/sentient_sentry/sentient_sentry/vision_node.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, DurabilityPolicy\n\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Float32\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n\n        # Declare parameters from params.yaml\n        self.declare_parameter('vision.camera_topic', '/image_raw')\n        self.declare_parameter('vision.face_cascade_path', '/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml')\n        self.declare_parameter('vision.image_width', 640)\n        self.declare_parameter('vision.image_height', 480)\n\n        self.camera_topic = self.get_parameter('vision.camera_topic').get_parameter_value().string_value\n        self.face_cascade_path = self.get_parameter('vision.face_cascade_path').get_parameter_value().string_value\n        self.image_width = self.get_parameter('vision.image_width').get_parameter_value().integer_value\n        self.image_height = self.get_parameter('vision.image_height').get_parameter_value().integer_value\n\n        # Initialize CvBridge for converting ROS Image messages to OpenCV images\n        self.bridge = CvBridge()\n\n        # Load the Haar Cascade for face detection\n        # This path might need to be adjusted based on your OpenCV installation\n        self.face_cascade = cv2.CascadeClassifier(self.face_cascade_path)\n        if not self.face_cascade.empty():\n            self.get_logger().info(f\"Loaded face cascade from: {self.face_cascade_path}\")\n        else:\n            self.get_logger().error(f\"Failed to load face cascade from: {self.face_cascade_path}\")\n\n        # QoS profile for camera image subscription\n        # Best Effort reliability is chosen to prioritize low latency over guaranteed delivery,\n        # which is typical for streaming sensor data where older data quickly becomes irrelevant.\n        image_qos_profile = QoSProfile(\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1, # Keep only the most recent message\n            durability=DurabilityPolicy.VOLATILE # Not relevant for transient data\n        )\n\n        # Create a subscription to the camera image topic\n        self.subscription = self.create_subscription(\n            Image,\n            self.camera_topic,\n            self.image_callback,\n            image_qos_profile # Apply the defined QoS profile\n        )\n        self.get_logger().info(f\"Subscribing to {self.camera_topic} with Best Effort QoS\")\n\n        # QoS profile for error publisher\n        # Reliable reliability ensures that all error messages are delivered to the ControlNode,\n        # which is crucial for stable feedback control.\n        error_qos_profile = QoSProfile(\n            reliability=ReliabilityPolicy.RELIABLE,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1,\n            durability=DurabilityPolicy.VOLATILE\n        )\n\n        # Create a publisher for the target error\n        self.publisher = self.create_publisher(\n            Float32,\n            '/sentry/target_error',\n            error_qos_profile # Apply the defined QoS profile\n        )\n        self.get_logger().info(\"Publishing to /sentry/target_error with Reliable QoS\")\n\n        self.get_logger().info('Vision Node has been started.')\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS Image message to OpenCV image (bgr8 format)\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            # Convert to grayscale for face detection (improves performance)\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            # Equalize histogram for better contrast (can help with varying lighting conditions)\n            gray = cv2.equalizeHist(gray)\n\n            # Detect faces in the grayscale image\n            faces = self.face_cascade.detectMultiScale(\n                gray,\n                scaleFactor=1.1, # How much the image size is reduced at each image scale\n                minNeighbors=5,  # How many neighbors each candidate rectangle should have to retain it\n                minSize=(30, 30) # Minimum possible object size. Objects smaller than that are ignored.\n            )\n\n            target_error_msg = Float32()\n            target_error_msg.data = 0.0 # Default to no error if no face is found\n\n            if len(faces) > 0:\n                # Assuming we want to track the largest face or the first one found\n                # For simplicity, we take the first detected face\n                x, y, w, h = faces[0]\n\n                # Calculate the center of the detected face\n                face_center_x = x + w / 2\n\n                # Calculate the image center\n                image_center_x = self.image_width / 2\n\n                # Calculate the horizontal error: difference between face center and image center\n                # Positive error means face is to the right of center, negative to the left.\n                target_error_msg.data = float(face_center_x - image_center_x)\n\n                # Optional: Draw a rectangle around the detected face for visualization\n                # For real-world applications, this might be done on a separate visualization stream\n                cv2.rectangle(cv_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            else:\n                self.get_logger().debug(\"No face detected.\")\n\n            # Publish the calculated error\n            self.publisher.publish(target_error_msg)\n\n            # Optional: Display the image with detected faces for debugging\n            # This is typically avoided on embedded systems for performance reasons\n            # cv2.imshow(\"Face Tracking\", cv_image)\n            # cv2.waitKey(1)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error in image_callback: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_node = VisionNode()\n    rclpy.spin(vision_node)\n    vision_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n\n"})}),"\n",(0,i.jsxs)(n.h4,{id:"4-control-node-control_nodepy",children:["4. Control Node (",(0,i.jsx)(n.code,{children:"control_node.py"}),")"]}),"\n",(0,i.jsx)(n.p,{children:"This node subscribes to the target error, implements a PID controller to generate angular velocity commands, and publishes these commands to the robot."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context"}),": ",(0,i.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/sentient_sentry/sentient_sentry/control_node.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, DurabilityPolicy\n\nfrom std_msgs.msg import Float32\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass ControlNode(Node):\n    def __init__(self):\n        super().__init__('control_node')\n\n        # Declare PID parameters from params.yaml\n        self.declare_parameter('pid.kp', 0.5)\n        self.declare_parameter('pid.ki', 0.0)\n        self.declare_parameter('pid.kd', 0.0)\n\n        self.kp = self.get_parameter('pid.kp').get_parameter_value().double_value\n        self.ki = self.get_parameter('pid.ki').get_parameter_value().double_value\n        self.kd = self.get_parameter('pid.kd').get_parameter_value().double_value\n\n        # PID control variables\n        self.last_error = 0.0\n        self.integral_error = 0.0\n        self.last_time = time.time()\n\n        # QoS profile for error subscription and cmd_vel publisher\n        # Reliable reliability is chosen for both to ensure precise control.\n        # It guarantees that no command or error signal is lost, which is critical for\n        # maintaining stable robot behavior.\n        control_qos_profile = QoSProfile(\n            reliability=ReliabilityPolicy.RELIABLE,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1,\n            durability=DurabilityPolicy.VOLATILE\n        )\n\n        # Create a subscription to the target error topic\n        self.subscription = self.create_subscription(\n            Float32,\n            '/sentry/target_error',\n            self.error_callback,\n            control_qos_profile # Apply the defined QoS profile\n        )\n        self.get_logger().info(\"Subscribing to /sentry/target_error with Reliable QoS\")\n\n        # Create a publisher for robot velocity commands\n        self.publisher = self.create_publisher(\n            Twist,\n            '/cmd_vel',\n            control_qos_profile # Apply the defined QoS profile\n        )\n        self.get_logger().info(\"Publishing to /cmd_vel with Reliable QoS\")\n\n        self.get_logger().info('Control Node has been started.')\n\n    def error_callback(self, msg):\n        current_error = msg.data\n        current_time = time.time()\n        dt = current_time - self.last_time\n\n        if dt == 0: # Avoid division by zero\n            return\n\n        # Proportional term\n        p_term = self.kp * current_error\n\n        # Integral term\n        self.integral_error += current_error * dt\n        i_term = self.ki * self.integral_error\n\n        # Derivative term\n        derivative_error = (current_error - self.last_error) / dt\n        d_term = self.kd * derivative_error\n\n        # Calculate the total control output\n        # We only control angular.z (yaw) to turn the robot\n        angular_velocity = p_term + i_term + d_term\n\n        # Create and publish the Twist message\n        twist_msg = Twist()\n        twist_msg.linear.x = 0.0  # No linear movement\n        twist_msg.linear.y = 0.0\n        twist_msg.linear.z = 0.0\n        twist_msg.angular.x = 0.0\n        twist_msg.angular.y = 0.0\n        twist_msg.angular.z = -angular_velocity # Negative because a positive error (face right) means turning left (negative angular.z)\n\n        self.publisher.publish(twist_msg)\n\n        # Update for next iteration\n        self.last_error = current_error\n        self.last_time = current_time\n\ndef main(args=None):\n    rclpy.init(args=args)\n    control_node = ControlNode()\n    rclpy.spin(control_node)\n    control_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsxs)(n.h4,{id:"5-launch-file-sentient_sentrylaunchpy",children:["5. Launch File (",(0,i.jsx)(n.code,{children:"sentient_sentry.launch.py"}),")"]}),"\n",(0,i.jsxs)(n.p,{children:["This launch file will start both the ",(0,i.jsx)(n.code,{children:"vision_node"})," and ",(0,i.jsx)(n.code,{children:"control_node"}),", loading parameters from ",(0,i.jsx)(n.code,{children:"params.yaml"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context"}),": ",(0,i.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/sentient_sentry/launch/sentient_sentry.launch.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import os\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    # Get the path to the sentient_sentry package\n    sentient_sentry_dir = get_package_share_directory('sentient_sentry')\n    config_dir = os.path.join(sentient_sentry_dir, 'config')\n    params_file = os.path.join(config_dir, 'params.yaml')\n\n    return LaunchDescription([\n        Node(\n            package='sentient_sentry',\n            executable='vision_node',\n            name='vision_node',\n            output='screen',\n            parameters=[params_file], # Load parameters from params.yaml\n            emulate_tty=True # Required for proper output buffering in Docker/Kubernetes environments\n        ),\n        Node(\n            package='sentient_sentry',\n            executable='control_node',\n            name='control_node',\n            output='screen',\n            parameters=[params_file], # Load parameters from params.yaml\n            emulate_tty=True\n        ),\n    ])\n"})}),"\n",(0,i.jsxs)(n.h4,{id:"6-update-setuppy",children:["6. Update ",(0,i.jsx)(n.code,{children:"setup.py"})]}),"\n",(0,i.jsxs)(n.p,{children:["To make your Python nodes executable, you need to add entry points in your ",(0,i.jsx)(n.code,{children:"setup.py"})," file within the ",(0,i.jsx)(n.code,{children:"sentient_sentry"})," package."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context"}),": ",(0,i.jsx)(n.code,{children:"/home/abdullahiqbal/Abdullah/hackathon-book-project/src/sentient_sentry/setup.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\nimport os\nfrom glob import glob\n\npackage_name = 'sentient_sentry'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        # Include all launch files in the 'launch' directory\n        (os.path.join('share', package_name, 'launch'), glob(os.path.join('launch', '*launch.[pxy][yell]'))),\n        # Include all config files in the 'config' directory\n        (os.path.join('share', package_name, 'config'), glob(os.path.join('config', '*.yaml'))),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Your Name', # TODO: Replace with your actual name\n    maintainer_email='your.email@example.com', # TODO: Replace with your actual email\n    description='A ROS 2 package for camera-based face tracking and robot control.',\n    license='Apache-2.0', # TODO: Choose an appropriate license\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            # Define your executable nodes here\n            'vision_node = sentient_sentry.vision_node:main',\n            'control_node = sentient_sentry.control_node:main',\n        ],\n    },\n)\n\n"})}),"\n",(0,i.jsx)(n.h3,{id:"\ufe0f-common-pitfalls-sim-vs-real",children:"\u26a0\ufe0f Common Pitfalls (Sim vs. Real)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulation"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"What works"}),": In Isaac Sim or Gazebo, odometry is often perfect, sensor noise is minimal or perfectly modeled, and lighting conditions are stable. Face detection might work flawlessly even with basic Haar cascades on clean textures. PID tuning can be done aggressively with immediate, predictable results. The Haar Cascade XML path is usually fixed."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reality"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"What fails"}),":","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Noise and Latency"}),": Real cameras suffer from motion blur (if the robot turns too fast), varying lighting, glare, and dead pixels. The ",(0,i.jsx)(n.code,{children:"/image_raw"})," topic might have higher latency or dropped frames, especially over Wi-Fi, impacting the ",(0,i.jsx)(n.code,{children:"VisionNode"}),"'s performance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Computational Constraints"}),": Running OpenCV face detection on an Edge Device (e.g., Jetson Orin Nano) with limited CPU/GPU and shared memory (typically 8GB) can introduce significant latency, leading to a delayed feedback loop and unstable control. Python's Global Interpreter Lock (GIL) can also be a bottleneck."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mechanical Backlash and Friction"}),": The real robot's motors and gears have friction and backlash, meaning small ",(0,i.jsx)(n.code,{children:"cmd_vel"})," commands might not result in immediate motion, leading to accumulated error or oscillations with high PID gains."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PID Tuning"}),": Gains that work in simulation will almost certainly cause instability (oscillations, overshooting, or sluggishness) on a real robot. The ",(0,i.jsx)(n.code,{children:"Kd"})," term is particularly sensitive to sensor noise."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Haar Cascade Path"}),": The ",(0,i.jsx)(n.code,{children:"face_cascade_path"})," might differ on your actual robot's OS, or the file might not be present if OpenCV is installed differently."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fix"}),":","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Pre-processing & Robustness"}),": Implement image pre-processing (e.g., dynamic range compression, adaptive histogram equalization) on the ",(0,i.jsx)(n.code,{children:"VisionNode"}),". Consider more robust (but computationally intensive) face detection models like MTCNN or SSD if computational resources allow, or switch to C++ for critical vision processing on embedded systems."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hardware Acceleration"}),": On Jetson platforms, leverage NVIDIA's DeepStream SDK or TensorRT to accelerate vision pipelines. If Python is a bottleneck, consider rewriting the ",(0,i.jsx)(n.code,{children:"VisionNode"})," in C++ with ",(0,i.jsx)(n.code,{children:"rclcpp"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Iterative PID Tuning (Ziegler-Nichols)"}),": Never use simulation PID gains directly. Start with very low gains and incrementally tune ",(0,i.jsx)(n.code,{children:"Kp"}),", ",(0,i.jsx)(n.code,{children:"Ki"}),", ",(0,i.jsx)(n.code,{children:"Kd"})," on the real robot. Introduce a deadband around the center to avoid constant micro-adjustments due to sensor noise."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Verify Cascade Path"}),": Explicitly check the ",(0,i.jsx)(n.code,{children:"face_cascade_path"})," on the target hardware and ensure it's correct and accessible. You might need to install ",(0,i.jsx)(n.code,{children:"opencv-haar-cascades"})," separately."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Emergency Stop"}),": ",(0,i.jsx)(n.strong,{children:":::danger"})," Always have a physical emergency stop for the robot. Untuned PID controllers or software glitches can cause unpredictable and potentially dangerous movements, leading to hardware damage or injury. Run safety checks before deployment. ",(0,i.jsx)(n.strong,{children:":::danger"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"-verification",children:"\ud83e\uddea Verification"}),"\n",(0,i.jsxs)(n.p,{children:["After compiling your ROS 2 workspace (using ",(0,i.jsx)(n.code,{children:"colcon build"}),") and sourcing your ",(0,i.jsx)(n.code,{children:"install/setup.bash"})," (or ",(0,i.jsx)(n.code,{children:"setup.zsh"}),"), you can launch your sentient sentry system."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch the Nodes:"}),"\nOpen a terminal, source your ROS 2 environment, and launch the system:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 launch sentient_sentry sentient_sentry.launch.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You should see output indicating that ",(0,i.jsx)(n.code,{children:"vision_node"})," and ",(0,i.jsx)(n.code,{children:"control_node"})," have started."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Verify Topics:"}),"\nOpen a ",(0,i.jsx)(n.em,{children:"new"})," terminal, source your ROS 2 environment, and check the active ROS 2 topics:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic list\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You should see ",(0,i.jsx)(n.code,{children:"/image_raw"}),", ",(0,i.jsx)(n.code,{children:"/sentry/target_error"}),", and ",(0,i.jsx)(n.code,{children:"/cmd_vel"})," among the listed topics."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Monitor Target Error:"}),"\nIn a new terminal, echo the ",(0,i.jsx)(n.code,{children:"/sentry/target_error"})," topic:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /sentry/target_error\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Initially, if no face is in view, you should see ",(0,i.jsx)(n.code,{children:"data: 0.0"}),". When a face appears (e.g., via a simulated face texture or a webcam if configured for ",(0,i.jsx)(n.code,{children:"/image_raw"})," publishing), you should see non-zero values indicating the horizontal offset."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Monitor Robot Commands:"}),"\nIn another new terminal, echo the ",(0,i.jsx)(n.code,{children:"/cmd_vel"})," topic:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /cmd_vel\n"})}),"\n",(0,i.jsxs)(n.p,{children:["If the robot is successfully detecting and tracking a face, you should observe ",(0,i.jsx)(n.code,{children:"angular.z"})," values changing (non-zero) when a face is off-center, and returning to ",(0,i.jsx)(n.code,{children:"0.0"})," when the face is centered. ",(0,i.jsx)(n.code,{children:"linear.x"})," should remain ",(0,i.jsx)(n.code,{children:"0.0"})," as we are only tracking rotation."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'This concludes the project guide for "Sentient Sentry".'})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);